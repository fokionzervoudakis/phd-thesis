\chapter{Conclusions and Future Work}
\label{chap:Conclusions_and_Future_Work}

This thesis describes a novel cascading verification method that uses composite reasoning over high-level system specifications and formalized domain knowledge to synthesize both system models and their desired behavioral properties. With cascading verification, model builders use a high-level DSL to encode system specifications that can be analyzed with model checking. Domain knowledge is encoded in OWL+SWRL and Prolog, which are combined to overcome their individual limitations. Synthesized DTMC models and PCTL properties are analyzed with the probabilistic model checker PRISM\@. Cascading verification was illustrated with a prototype system that verified the correctness of UAV mission plans. An evaluation of this prototype revealed non-trivial reductions in the size and complexity of input system specifications compared to the artifacts synthesized for PRISM\@.

The remainder of this chapter is structured as follows. Section~\ref{sec:Contributions} reiterates our contributions to the state of the art in semantic model checking. Section~\ref{sec:Future_Work} discusses directions for future work. Two specific areas of research, involving network centric and annotation-guided model checking, are described in Section~\ref{sec:Network_Centric_Model_Checking} and Section~\ref{sec:Annotation_Guided_Model_Checking}, respectively.

\section{Contributions}
\label{sec:Contributions}

With cascading verification, we claim several contributions to semantic model checking, a method that leverages semantic reasoning over domain knowledge to augment the model checking process. Unlike related work, our method synthesizes both system models \emph{and} behavioral properties for probabilistic model checking. Cascading verification is underpinned by a composite DL- and LP-based inference mechanism that overcomes expressive and reasoning limitations in the ontology language OWL\@. By using our method to verify UAV missions, we highlight the potential portability of cascading verification and, ultimately, semantic model checking, which has thus far been applied exclusively to the Web services domain.

We illustrate cascading verification with a prototype system that verifies the correctness of~58 UAV mission plans; the development of those plans is structured with DSM\@. On average, our prototype synthesizes PRISM code that is 3.127 and 4.490 times greater than the size of YAML input with regard to LOC and tokens, respectively. For traffic-surveillance missions, the prototype realizes even bigger reductions with synthesized PRISM code that is 4.522 and 8.087 times greater than the size of YAML input with regard to LOC and tokens, respectively. These results provide preliminary evidence of non-trivial reduction in the effort required to produce mission models and properties.

LOC- and token-based metrics are used to evaluate the abstraction, i.e., the reduction in modeling complexity, afforded by our prototype. In addition to enhanced abstraction, the prototype augments PRISM's verification capabilities and thereby enhances the effectiveness of probabilistic model checking. We evaluate effectiveness by presenting errors that can only be effectively eliminated with the automated synthesis of PRISM artifacts. We also evaluate the utility of the DTMC and PCTL artifacts synthesized by our prototype.

\section{Future Work}
\label{sec:Future_Work}

We have identified several promising directions for future work. Composite CVC inferences are currently unidirectional, with Prolog facts derived from knowledge encoded in OWL+SWRL\@. The effects of this \emph{pipeline} architecture were particularly pronounced in Section~\ref{sec:Classification_with_Prolog}, where semantic reasoning underpinned Prolog-based classifications, which subsequently impacted the synthesis of DTMC and PCTL artifacts. While conceptually and practically appealing, an inference pipeline constrains the reasoning process from refining Prolog inferences with ontological knowledge, and increases the potential for knowledge duplication. We aim to address these limitations by developing a knowledge representation framework that can support more flexible, iterative reasoning.

A second issue pertains to the artifacts that constitute the CVC knowledge base including CEMO, the Prolog rule-base, and the DTMC and PCTL templates. These artifacts should be extensible to reflect changes in domain knowledge. Extensions should in turn be verifiable to ensure that domain knowledge remains consistent across the entire knowledge base. This requirement provides impetus for the development of a mechanism that will automate the consistency management process.

We also intend to further the evaluation of our method and prototype by enhancing the sophistication of the mission specification language and domain model presented in this thesis. And we intend to confirm the portability of cascading verification by applying our method to other significant application domains. We expect a more robust evaluation process to facilitate the abstraction and formal specification of the connections that link different technologies in the context of our method. Once formalized, these connections will likely support the consistency management process described in the preceding paragraph, and the development of a domain-agnostic compiler. Such a compiler would receive as input the artifacts and connections that constitute a domain-specific knowledge base and thereby eliminate the current de facto requirement for bespoke implementations of the CVC\@.

Our work has yet to address the problem of tracing PRISM results back to the underlying system specifications. Without traceability, the analysis provided by PRISM may be incomprehensible to model builders~\cite{Combemale_2011}. In the context of the YAML DSL, traceability would serve to disambiguate the relationship between syntactic rules and operational semantics, which are defined with OWL and the PRISM language, respectively. Once formalized, elements of this relationship will likely parallel the connections described in the preceding paragraph.

Future work discussed thus far is closely related to the research, development, evaluation and outcomes presented in this thesis. The following sections present research directions that are more expansive in scope.

\subsection{Network-Centric Model Checking}
\label{sec:Network_Centric_Model_Checking}

Network-Centric Operations (NCO) is a doctrine that leverages information technology to improve the effectiveness and efficiency of military operations~\cite{Wilson_2007}. NCO is underpinned by contemporary socio-technological advancements, and enabled by ``a high-performance information grid, access to all appropriate information sources, weapons reach and maneuver with precision and speed of response, value-adding C2 processes---to include high-speed automated assignment of resources to need---and integrated sensor grids closely coupled in time to shooters and C2 processes.''~\cite{Cebrowski_1998} When combined, these elements support \emph{speed of command}, the process by which superior information is turned into competitive advantage. Speed of command can be substantially enhanced when command-and-control processes are automated. Enhanced speed of command accelerates the \emph{observe, orient, decide and act} (OODA) loop, which denies the enemy operational pause. Regaining this time amplifies the effects associated with speed of command, resulting in an accelerated rate of change that leads to enemy lock-out.

By automating the organization and utilization of complex operational knowledge, cascading verification could support the analysis and deployment of mission plans comprising asset configurations derived from real-time operational data including asset location, fuel and weapon statuses. Near real-time coupling of mission verification and deployment has the potential to yield a near real-time OODA loop, which will inevitably be susceptible to network and processing speed latencies. Addressing the impact of latency on mission correctness in the context of NCO constitutes an interesting research direction.

\subsection{Annotation-Guided Model Checking}
\label{sec:Annotation_Guided_Model_Checking}

With our prototype implementation of cascading verification, model builders encode mission specifications in a YAML DSL\@. Notwithstanding the inherent advantages of YAML, the introduction of a novel declarative formalism can be associated with potential disadvantages. Declarative programming that is based on first-order or higher-order logics does not cope well with temporal systems~\cite{Lloyd_1994}. This limitation, which derives from the static model theory that defines standard logics, impacts the level of expressivity afforded to mission developers. Furthermore, the novelty of our formalism increases the complexity for potential adopters, and distances the project from real-world grounding, thereby potentially compromising the utility and evaluation of our prototype.

URBI and the Robot Operating System (ROS) are sophisticated, cross-platform and open-source frameworks that support robotic software development. Both frameworks are interoperable with established programming languages, including C++ and Java, that mitigate the aforementioned limitations. With Java-encoded missions, Java annotations could be used by model builders to embed domain knowledge in executable code, and thereby guide the automated synthesis of verification artifacts. Annotation-based verification frameworks and other related work indicate this to be a promising direction for future work~\cite{Chalin_2005,Holmgren_2005,Ferreira_2007}. Development and evaluation of the proposed annotation framework would be informed by, and thereby benefit from, our experience with the existing YAML DSL\@.
